---
title: 'ST 552: Lab 2'
date: "January 18, 2023"
output:
  html_document:
    df_print: paged
---

## Outline

- Matrix algebra in R
- Practice problems
- Simulation of normal random variables in R

## Matrix algebra in R

Go to [https://www.statmethods.net/advstats/matrix.html](https://www.statmethods.net/advstats/matrix.html), which has a list of matrix functions in R you can use in lab and on your second homework. Throughout the lab you will see a variety of different ways to create, change, and manipulate matrices.

#### Creating matrices

First, let's create two matrices, $\mathbf{A}_{3\times 4}$ and $\mathbf{B}_{4\times 2}$:

```{r}
A <- matrix(1:12, nrow=3)
B <- matrix(rep(c(0,1), times=4), nrow=4)

A
B
```

Matrices can also be created by binding together vectors (horizontally or vertically):

```{r}
cats <- 1:4
dogs <- c(4,6,3,1)
animals <- rbind(cats, dogs)
animalsv <- cbind(cats, dogs)

animals
animalsv
```

Matrices can also be bound together to create bigger matrices (as long as the dimensions work):

```{r}
rbind(A, animals)
```


#### Matrix multiplication and addition

> Question: Do these matrices have compatible dimensions to calculate $\mathbf{AB}$? How about $\mathbf{BA}$?

*Answer: (Yes, No)*

Let's try in R to confirm (uncomment the lines). What happens?

```{r}
# A %*% B
# B %*% A
```

Note that the `*` operator without the percent signs on either side will do element-wise multiplication, which is not what we typically want for matrix multiplication. Will this work for `A * B`?

*(No, because $\mathbf{A}$ and $\mathbf{B}$ have different dimensions)*

Let's create another matrix $\mathbf{C}$ with the same dimension as $\mathbf{A}$ to check:

```{r}
# create a 3x4 matrix of all 0s
# then change the i=1,j=2 (first row, second column) element to be 1
C <- matrix(0, nrow=3, ncol=4)
C[1,2] <- 1

A * C
```

We can also add matrices if the dimensions are appropriate (uncomment the lines to test):

```{r}
# A + C # works
# A + B # does not work
```

#### Transpose, Trace, and Inverse

We can find the transpose and trace of any matrix.

```{r}
t(A) # transpose
sum(diag(A)) # trace
```

We can find the inverse of matrices that are invertible. Non-square matrices do not have an inverse. Let's try that for $\mathbf{A}$, which is not square, and see the error message (uncomment the line).

```{r}
# solve(A)
```

Now let's create a small square matrix $\mathbf{D}_{2\times 2}$ that happens to be invertible, and invert it:

```{r}
D <- matrix(c(4,2,10,15), nrow=2, byrow=TRUE)
solve(D)
```

We could also have done this "by hand" using the formula from slide 20 of Lecture 4:

```{r}
D.a <- 4
D.b <- 2
D.c <- 10
D.d <- 15
1/(D.a*D.d - D.b*D.c) * matrix(c(D.d, -D.b, - D.c, D.a), nrow=2, byrow=TRUE)
```

The `solve()` function is handy for bigger matrices.

#### Eigenvectors and eigenvalues

The `eigen()` function allows us to find both:

```{r}
eigen(D)
```

```{r}
# or, if we want to be able to extract the particular numbers
y <- eigen(D)
y$values
y$vectors
```

Uncomment the following line to view the error message:

```{r}
# eigen(A) # error for wrong dimensions
```

#### Practice problems:

The next portion of the lab you can try on your own. Write code to do the following:

1. Create the following matrices (note: it may be helpful to compile the document so you can see what the matrix looks like):
    - $\mathbf{I}_{10\times 10}$ (for a hint, see the link at the beginning of the lab)
    - $\mathbf{G}=\begin{pmatrix}1 & 0 & 0 & \cdots & 0 \\ 0 & 2 & 0 & \cdots & 0 \\ 0 & 0 & 3 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots\\0 & 0 & 0 & \cdots & 10\end{pmatrix}$ 
    - $\mathbf{X}=\begin{pmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3 \\ \vdots & \vdots \\ 1 & 10 \end{pmatrix}$ 

2. Calculate:
    - $\mathbf{X}^T$
    - $\mathbf{G}^{-1}$
    - $\mathbf{X}^T\mathbf{X}$
    - $\mathbf{X}^T\mathbf{G}$

3. Find $\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$ and $\mathbf{H}^2=\mathbf{HH}$. Are they the same (see note below)? More to come in lecture about what these are and why..

    *Note/aside*: There are a few ways to check if matrices are equal. You can use the double equal sign, e.g. `A == A` which should return a matrix with all elements `TRUE`. You can also see if `A - A` is equal to a matrix with all zeroes. Or you can just visually inspect. If you try to use the first two approaches, it will look like $\mathbf{H}$ and $\mathbf{H}^2$ have some non-equal elements. I believe this is because of a numerical optimization approach R uses to find the inverse. If you look at the differences between the matrices, you will see that the values are like `-1.1e-17`, so very close to 0. Mathematically they should be exactly equal to 0.

4. Bonus: is $\mathbf{H}$ symmetric? How about $\mathbf{I}-\mathbf{H}$?

## Simulation of normal random variables in R

To simulate a realization of $n$ independent Normal random variables with mean 0 and standard deviation 1:

```{r}
n <- 10
rnorm(n, mean=0, sd=1) # you can omit the mean and sd arguments for the standard normal - these are the defaults
```

`rnorm()` has arguments `mean` and `sd` if you need a different mean and standard deviation.

Want dependence? Start with uncorrelated observations, and transform them or use the function `rmvnorm()` in the `mvtnorm` package.